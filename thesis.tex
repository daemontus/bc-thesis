\documentclass[12pt,oneside,draft]{fithesis2}
\usepackage[english]{babel}       % Multilingual support
\usepackage[utf8]{inputenc}       % UTF-8 encoding
\usepackage[T1]{fontenc}          % T1 font encoding
\usepackage[                      % A sans serif font that blends well with Palatino
  scaled=0.86
]{berasans}
\usepackage[                      % A tt font if you do not like LM's tt
  scaled=1.03
]{inconsolata}
\usepackage[                      % Clickable links
  plainpages = false,               % We have multiple page numberings
  pdfpagelabels                     % Generate pdf page labels
]{hyperref}
\usepackage{blindtext}            % Lorem ipsum generator

 % Algorithms
\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
\usepackage{algpseudocode}

\thesislang{en}                   % The language of the thesis
\thesistitle{Parameter Synthesis from Hypotheses Formulable in CTL Logic}       % The title of the thesis
\thesissubtitle{Bachelor Thesis}  % The type of the thesis
\thesisstudent{Samuel Pastva}          % Your name
\thesiswoman{false}                % Your gender
\thesisfaculty{fi}                % Your faculty
\thesisyear{Spring \the\year}     % The academic term of your thesis defense
\thesisadvisor{John Foo, Ph.D.}   % Your advisor


% Macros for definitions
% Assumptions
\newcommand{\as}[1][]{\ensuremath{\mathcal{A}_{#1}}}
\newcommand{\asemt}{\as[\perp]}
\newcommand{\ASphi}{\ensuremath{AS_{\mathcal K}^\phi}}

% Algorithm
\newcommand{\method}[1]{\textnormal{\textsc{#1}}}

%True, false
\newcommand{\true}{\ensuremath{\texttt{tt}}}
\newcommand{\false}{\ensuremath{\texttt{ff}}}

% Kripke
\newcommand{\ks}[1][]{\ensuremath{\mathcal{K}_{#1}}}
\newcommand{\kf}[2]{\ensuremath{\mathcal{F}^{#2}_{#1}}}
\newcommand{\fullKs}{\ensuremath{ \ks = (\params, S, S_0, \trans{}, L) }}
\newcommand{\fullKf}[2]{\ensuremath{ \kf{#1}{#2} = (f, \params, S_{#1}, I_{i}, \trans{}_{#1}, L_{#1}) }}
\newcommand{\trans}[1]{\stackrel{#1}{\rightarrow}}
\newcommand{\params}{\mathcal{P}}

%Temporal
\newcommand{\eu}[2]{\ensuremath{\mbox{E} #1 \mbox{U} #2 }}
\newcommand{\au}[2]{\ensuremath{\mbox{A} #1 \mbox{U} #2 }}
\newcommand{\ex}[1]{\ensuremath{\mbox{EX} #1}}
\newcommand{\ax}[1]{\ensuremath{\mbox{AX} #1}}
\newcommand{\ef}[1]{\ensuremath{\mbox{EF} #1}}
\newcommand{\af}[1]{\ensuremath{\mbox{AF} #1}}
\newcommand{\eg}[1]{\ensuremath{\mbox{EG} #1}}
\newcommand{\ag}[1]{\ensuremath{\mbox{AG} #1}}

\begin{document}
  \FrontMatter                    % The front matter
    \ThesisTitlePage                % The title page
    \begin{ThesisDeclaration}       % The declaration
      \DeclarationText
      \AdvisorName
    \end{ThesisDeclaration}
    \begin{ThesisThanks}            % The acknowledgements (optional)
      I would like to thank my supervisor\,\dots
    \end{ThesisThanks}
    \begin{ThesisAbstract}          % The abstract
      The aim of the bachelor work is to provide\,\dots
    \end{ThesisAbstract}
    \begin{ThesisKeyWords}          % The keywords
      keyword1, keyword2\,\dots
    \end{ThesisKeyWords}
    \tableofcontents                % The table of contents
%   \listoftables                   % The list of tables (optional)
%   \listoffigures                  % The list of figures (optional)
  
  \MainMatter                     % The main matter
    \chapter{Introduction}          % Chapters
	Lorem Ipsum \cite{testCite}

	\chapter{Terms and Definitions}
		
		\section{Kripke Structure}
	
			As an input of our algorithm, we expect a parametrized Kripke Structure as defined in [CITE]. Parametrized Kripke Structure is a tuple $\fullKs$ where
			
			\begin{itemize}
				\item $\params$ is a finite set of parameters (all possible parameter valuations)
				\item $S$ is a finite set of states
				\item $S_0 \subseteq S$ is a set of initial states
				\item $\trans{} \subseteq S \times \params \times S$ is a total transition relation labeled by parameter valuations 
				\item $L: S \rightarrow 2^{AP} $ is a labeling function from states to sets of atomic propositions which are true in such states
			\end{itemize} 		
			
			We write $ s \trans{p} s' $ when $ (s, p, s') \in \trans{} $. We also write $ s \trans{} s'$ when $\exists p \in \params. (s, p, s') \in \trans{} $. Note that fixing a valuation $p \in \params$ reduces the Parametrized Kripke Structure $\ks$ to concrete, non-parametrized Kripke Structure $\ks(p) = (S, S_0, \trans{p}, L)$.
						
		\section{CTL Logic}
		
			In order to correctly express various hypotheses in systems biology, the idea of branching time is need. Examples of such hypotheses are given in section [Case Study]. Therefore, this work uses the Computation Tree Logic(CTL) as means of hypotheses formulation. 

			CTL syntax is defined inductively upon finite set of atomic propositions:
			
			\begin{equation}
				\varphi ::= 
					true \mid
					false \mid
					Q \mid 
					\neg \varphi_1 \mid 
					\varphi_1 \wedge \varphi_2 \mid
					\ex{\varphi_1} \mid
					\eu{\varphi_1}{\varphi_2} \mid
					\au{\varphi_1}{\varphi_2}			
			\end{equation}
			
			Sometimes, we will use parentheses to make bigger formulas easily readable, but they will in no way be used to modify the meaning of formula or priority of operators.
			
			Note	 that there are also other temporal operators in standard CTL definition. We do not implement those directly in our algorithm. However, we use following equations to transform any general CTL formula prior to computation, so that it only uses operators supported in our algorithm. This way we can achieve a concise algorithm and also support whole CTL logic.
			
			\begin{itemize}
				\item $ \ax{\varphi} = \neg \ex{ \neg \varphi } $
				\item $ \ef{\varphi} = \eu{(true)}{\varphi} $
				\item $ \eg{\varphi} = \neg \au{(true)}{\neg \varphi} $
				\item $ \af{\varphi} = \au{(true)}{\varphi} $
				\item $ \ag{\varphi} = \neg \eu{(true)}{\neg \varphi}$
			\end{itemize}
			
			Note that all of these transformations also preserve number of temporal operators in a formula. 
			
			Other boolean operators like implication or equivalence can also be derived using similar transformations. 			
			
			Let $\varphi$ be a CTL formula. We write $cl(\varphi)$ to denote the set of all sub-formulas of $\varphi$ and $tcl(\varphi)$ to denote the set of all temporal sub-formulas of $\varphi$. By $|\varphi|$ we denote the size of formula $\varphi$.
			
			We assume standard CTL semantics over non-parametrized Kripke structures as defined in [cite].

		\section{Parameter Synthesis Problem}
			
			Parameter synthesis problem is defined in following way. Suppose we are given a parametrised Kripke structure $\fullKs$ and a CTL formula $\varphi$. For each state $s \in S$ let $ P_s = \{ p \in \params \mid s \models_{\ks(p)} \varphi \} $,where $s \models_{\ks(p)} \varphi$ denotes, that $\varphi$ is satisfied in the state $s$ of $\ks(p)$. The parameter synthesis problem requires to compute the function $\mathcal{M}_{\varphi}^{\ks} : S \rightarrow 2^\params $ such that $\mathcal{M}_{\varphi}^{\ks}(s) = P_s$. Often we are especially interested in computing the set of all parameters for which the property holds in some initial states $\cap_{s\in S_0}\mathcal{M}_\varphi^{\mathcal K}(s)$. We will sometimes omit the $\varphi$ and $\ks$ when they are clear from the context.	
				
		\section{CTL logic and model approximation}
		
			In model checking, some modeling approaches suffer from over or under approximation. We say that model is over-approximated when all feasible transitions are contained in the model, but it can also contain transitions that are not feasible in the situation the model is describing. Symmetrically, we say that model is under-approximated when all transitions in the model are feasible in the modeled situation, but not all feasible transitions has to be contained in the model. 
			
			It is important to discuss this relationship between CTL and approximated models, because it is much more complicated compared to linear-time logic since CTL allows for universal and existential quantification mixing.
			
			We say that CTL formula is \emph{universal} or that it belongs to ACTL when it only contains universal temporal operators and no negation. Symmetrically, we say that CTL formula is \emph{existential} or that it belongs to ECTL when it only contains existential temporal operators and no negation.
			
			Observe that the truth of ACTL properties is preserved in over-approximated models. In other words, if an ACTL property holds in an over-approximated model, it must also hold in the original model. However, their falsity cannot be guaranteed, because the false transitions may introduce paths that falsify the property in states where it would be normally true. Similarly, the falsity of ECTL properties in over-approximated models is preserved but the truth is not. In this case, the existence of false transitions can introduce states where ECTL property holds solely due to these false paths.
			
			Symmetrically, for under-approximated models, the falsity of ACTL and the truth of ECTL is preserved. But due to similar arguments, we can't say anything about their counterparts.
			
			If we allow full CTL, in general, we can't make any assumptions about results obtained from either under- or over-approximated systems. This is caused by mixing of existential and universal quantification which leads to results which may be spurious and incomplete at the same time. Therefore, no conclusions can be made without further investigation and validation of such results.
			
		\section{Kripke Fragments}
		
			Due to the state space explosion, given parametrised Kripke structure can be very large and therefore impossible to fit into memory of one computer. In order to solve parameter synthesis problem for such structures, we have to distribute the state space across several computational nodes. To this end, we introduce the notion of parametrised kripke fragments.
			
			A parametrised Kripke structure $\ks$ can be divided into several parametrised Kripke fragments $\kf{1}{\ks}, \kf{2}{\ks}, \cdots , \kf{N}{\ks}$ using a partition function $f$. Parametrised Kripke fragment with identifier $i$ over Kripke structure $\fullKs$ is then defined as a tuple $\fullKf{i}{\ks}$ where:
			
			\begin{itemize}
				\item $f : S \rightarrow \{1, \cdots , N \}$ is a partition function from all states to fragment identifiers
				\item $\params$ is a finite set of all parameters
				\item $S_{i} = \{ s \in S \mid f(s) = i \vee \exists s'\in S. ((s \trans{} s' \vee s' \trans{} s) \wedge f(s') = i \}$ is a subset of original state space which belongs to this fragment
				\item $I_{i} = \{ s \in S_0 \mid s \in S_{i} \} $ is a set of all fragments initial states
				\item $\trans{}_{i} = \{ (s, p, s') \in \trans{} \mid s \in S_{i} \wedge s' \in S_{i} \wedge (f(s) = i \vee f(s') = i) \}$ is a subset of original transition function reduced to only relevant states (not required to be total)
				\item $L_{i} = \{ (s, l) \in L \mid f(s) = i \}$ is a labeling subset of original labeling function relevant to this fragment
			\end{itemize}
						
			In the following text, we will often omit the superscript $\ks$ if it is clear from context.
			
			Intuitively, Kripke fragment represents a subset of the original kripke structure defined by the partition function. It contains all nodes specified by the partition function with all their direct successors and predecessors and all corresponding transitions. 			
			
			We define a set of border states $border(\kf{i}{\ks}) = \{ s \in S_{i} \mid f(s) \neq i \}$. Intuitively, these states represent the remaining portion of the state space which is stored in memory of other processes and is not directly accessible. We say that state is an \emph{internal state} if it is not a border state. We also define a set of cross edges as $cross(\kf{i}{\ks}) = \{ (s, p, s') \in \trans{}_{i} \mid s' \in border(\kf{i}{\ks}) \vee s \in border(\kf{i}{\ks}) \}$. Intuitively, these are edges leading from border states to internal states or vice versa. 
			
			Note that for a constant partition function $f(s) = 1$ and any given kripke structure, the partitioning results in one fragment with an unchanged state space and an unchanged transition relation (The sets of border states and cross edges are empty for the resulting fragment). Similarly, for every Kripke structure, there exists a partition function for which $N = |S|$ and every resulting fragment contains only one internal state. Under such partitioning, all edges are cross edges. 
			
			In worst case (connected graphs), such partitioning results in fragments with $|S| - 1$ border states and one internal state (there is no way to achieve higher state count in fragment than in the original structure). Therefore we can assume that $\sum\limits_{i=1}^N | S_i | \leq |S|^2 $, hence the number of introduced border states is at worst quadratic in terms of original state space. The number of internal states remains the same.

			This increase seems rather high, however, it is important to note that this also requires one process for each original state. In real life scenarios, the number of states per process is usually much higher. Also note that the representation of border state in memory is usually much simpler (and smaller) than representation of internal state, so even a distribution with high border state count can be beneficial in terms of memory consumption. 
						
			In terms of edges, in the worst case, each edge has to be contained in two fragments (where either of the end states is an internal state), therefore the total number of edges is at worst doubled.			
			
			The number of border states and cross edges is also highly dependent on the partition function. It is usually best to design the partition function with specific model or modeling approach in mind in order to achieve optimal workload distribution. We will discuss different partition functions later in the text. 
			
			For each kripke fragment $\kf{i}{}$, we define a relation $successors \subseteq S_{i} \times 2^\params \times S_{i} $ to denote the set of colors for which there exist a transition from the first to the second state.
			
			\begin{center}
				$successors = \{(s, P, s') \mid P = \{p \in \params \mid (s, p, s') \in \trans{}_{i} \}\} $
			\end{center}
			
		\section{Assumption Semantics}
		
			Classic interpretation of CTL formulas is not adequate for Kripke fragments. In order to accommodate for possible non-totality and distributed nature of the Kripke fragments over Kripke structure $\fullKs$, we introduce the assumption function $\as : \params \times S \times cl(\varphi) \rightarrow Bool $. The values $\as(s, p, \varphi_1)$ are called assumptions. We use the notation $\as(p, s, \varphi_1) = \perp $ to say that the value of $\as(s, p, \varphi_1)$ is undefined. By $\asemt$ we denote assumption function which is undefined for all inputs.
			
			Intuitively, $\as(s, p, \varphi_1) = \true$ when we can assume that $\varphi_1$ holds in state $s$ for parameter valuation $p$, $\as(s, p, \varphi_1) = \false$ when we can assume that $\varphi_1$ does not hold in state $s$ for parameter valuation $p$ and $\as(s, p, \varphi_1) = \perp$ when we cannot assume anything about validity of $\varphi_1$ in state $s$ for parameter valuation $p$.
			
			Undefined values are important in CTL semantics over distributed fragments, since such values can be used in places where validity of formula cannot be computed because it depends on information stored in another fragment which has not been received yet. However, a correct model checking algorithm should always provide a definitive answer for all states and parameter valuations.
			
			For a Kripke fragment $\fullKf{i}{}$ and a formula $\varphi$, the assumption function is defined inductively on the structure of the formula $\varphi$:
			
			\begin{center}								
				\[\as(p, s, Q) = \left\{ 
  					\begin{array}{l l}
					    \true & \quad Q \in L(s)\\
					   	\false & \quad otherwise\\
				  \end{array} 
				\right.\]
			\end{center}
			
			\begin{center}								
				\[\as(p, s, \neg \varphi_1) = \left\{ 
  					\begin{array}{l l}
					    \true & \quad \as(p, s, \varphi_1) = \false\\
					   	\false & \quad \as(p, s \varphi_1) = \true\\
					   	\perp & \quad otherwise\\
				  \end{array} 
				\right.\]
			\end{center}
			
			\begin{center}								
				\[\as(p, s, \varphi_1 \wedge \varphi_2) = \left\{ 
  					\begin{array}{l l}
					    \true & \quad \as(p, s, \varphi_1) = \true $ and $ \as(p, s, \varphi_2) = \true\\
					   	\false & \quad \as(p, s \varphi_1) = \false $ or $ \as(p, s, \varphi_2) = \false\\
					   	\perp & \quad otherwise\\
				  \end{array} 
				\right.\]
			\end{center}			
			
			\begin{center}								
				\[\as(p, s, \ex{\varphi_1}) = \left\{ 
  					\begin{array}{l l}
					    \true & \quad \exists s' \in S : s \trans{p} s' \wedge \as(p, s', \varphi_1) = \true\\
					   	\false & \quad \forall s' \in S : s \trans{p} s' \Rightarrow \as(p, s', \varphi_1) = \false\\
					   	\perp & \quad otherwise\\
				  \end{array} 
				\right.\]
			\end{center}			
			
		
    \chapter{Algorithm}
    
    	In this chapter, we describe the distributed algorithm that computes the assumption function $\as$. 
    	
    	\section{Distributed Environment}
    	
		In this section, we briefly describe the distributed environment assumed by our algorithm, in order to prevent any possible confusion.
		
		We assume a distributed environment with fixed number of reliable processes connected by reliable, order-preserving channels (The order preservation can be relaxed to some extent). We also assume that each process has a fixed identifier and the set of all process identifiers is equal to the result set of the partition function. Each process can communicate directly (using the function \method{Send}) with any other process (assuming it knows other process's identifier). We assume that each message can be transmitted in $O(1)$ time and all messages that can't be processed directly are stored in a queue until they can be processed.
		
		Several parts of the algorithm do not have explicit termination (they terminate by reaching deadlock - no messages are exchanged between processes). In such cases, suitable termination detection algorithm is employed to detect this deadlock and terminate computation properly. Our implementation uses Safra's algorithm [CITATION] for this purpose, but the related code has been skipped for easier readability.
    	
		The algorithm is broken into two main parts. First describes the general outline of algorithm and is similar to classic CTL model checking. Second part describes how each of the supported temporal operators is processed. This part contains more detailed description of inter-process communication and operator specific data structures. To better reflect the distributed nature of the algorithm, description of each temporal operator routine is divided into three parts: Process variables, Initialization and Message handler. First section describes data structures stored in process memory available during whole computation. Initialization section is executed exactly once and no messages can be received until it's finished. Message handler defines what should happen when message is received.  
		
		\section{Algorithm outline}
		
			The main idea of the algorithm is described in REFERENCE and resembles other CTL model checking algorithms.
			
			\begin{algorithmic}[1]
				\Procedure{CheckCTL}{$ \phi, \fullKs $}
					\State $ \as \gets \{ (p, s, \alpha, \true) \mid  p \in \params \wedge \alpha \in L(s) \}$ 
					\ForAll { $ i < | \phi | $ }					
						\ForAll { $ \psi $ \textbf{in} $ cl(\phi) $ \textbf{where} $ | \psi | = i $ }
							\State $ \as \gets \Call{CheckFormula}{\psi, \ks, \as } $
						\EndFor
					\EndFor
				\EndProcedure			
			\end{algorithmic} 
			
			The algorithm starts by initializing the assumption function using the labeling function defined in kripke fragment. After that, it traverses the structure of formula, starting from smallest formulas and uses computed results to process more complex formulas. Function \method{CheckFormula} computes all states and colors where formula $\psi$ holds and returns assumption function updated accordingly. This is done using the local information contained in given kripke fragment, assumptions previously computed for smaller formulas and also by communicating with other processes. Note that only assumptions relevant for particular process are computed and returned (each process has information only about it's own state space).
    	
    	\section{Common operations}
    		
    		In this section, we define functions used to simplify the algorithm description. Let us fix a formula $\phi$ and a parametrised kripke fragment $\fullKs$ as an input of the algorithm.
			
			Intuitively, function $validStates : cl(\phi) \times \ASphi \rightarrow S \times 2^\mathcal{P} $ computes a set of states and parameters where truth of given formula is assumed. It is also responsible for handling of boolean operators, since these can be computed without any inter-process communication. 
			
			\begin{center}
				$validStates(\phi_1 \wedge \phi_2, \as) = \{ (s, p) \mid \as(s, p, \phi_2) = \true \wedge \as(s, p, \phi_2) = \true \} $
			\end{center}
			
			\begin{center}
				$validStates(\phi_1 \vee \phi_2, \as) = \{ (s, p) \mid \as(s, p, \phi_2) = \true \vee \as(s, p, \phi_2) = \true \} $
			\end{center}
			
			\begin{center}
				$validStates(\neg\phi, \as) = \{ (s, p) \mid \as(s, p, \phi) = \false \} $
			\end{center}
			
			\begin{center}
				$validStates(\true, \as) = \{ (s, p) \mid s \in S \wedge p \in \params \} $
			\end{center}

			\begin{center}
				$validStates(\false, \as) = \emptyset $
			\end{center}
						
			\begin{center}
				$validStates(\phi, \as) = \{ (s, p) \mid \as(s, p, \phi) = \true \} $
			\end{center}

			Another useful function is $validColours : cl(\phi) \times S \times \ASphi \rightarrow 2^\mathcal{P}	$ which returns a set of parameters for which given formula is assumed to be true in given state. 
			
			\begin{center}
				$validColours(\phi, state, \as) = \{ p | (state, p) \in validStates \}$
			\end{center}
			
			We also define function $update: \ASphi \times S \times 2^{\params} \times tcl(\phi) \rightarrow \ASphi $ which takes current assumptions, a state, set of parameters and a formula and returns assumptions updated so that for all parameters of the given set, formula holds in given state.
			
			\begin{algorithmic}
				\State $ update(\as, state, colours, \phi): $
				\ForAll { $ p \in colours $}
					\State Set $ \as(state, p, \phi) = \true $
				\EndFor 
			\end{algorithmic}
			
		\section{Temporal Operators}
    
    		In this section, we describe how the \method{CheckFormula} is implemented for each of the temporal operators. Note that all of the following algorithms has implicit termination and therefore needs a proper termination detection algorithm to correctly terminate.
    		
	   		\subsection{Exist Next Operator}
   			   			
				\begin{algorithmic}[1]
				\State $ \textbf{Process variables:} $
				\State $ \ks = (id, f, \mathcal{P}, S, I, \trans{p}, L) $ \Comment{Kripke fragment}
				\State $ \phi = \ex{\phi_1}  $ \Comment{CTL formula}
				\State $ \as $ \Comment{Initial assumption function}
				\Procedure{Init}{}
					\ForAll {$(state, colSet)$ \textbf{in} $ validStates(\phi_1, \as) $}
						\ForAll { $ (pred, tranCol) $ \textbf{in} $ predecessors(state) $ }
							\State $ \Call{Send}{f(state), (pred, colSet \cap tranCol)} $
						\EndFor
					\EndFor
				\EndProcedure
				\Procedure{Receive}{$colSet, to$}
					\State $ \as \gets update(\as, to, colSet, \phi)$
				\EndProcedure
				\end{algorithmic}	
				
				The simplest of temporal operators is the \ex{} operator. During initialization, all states and colors where $\phi$ holds are computed. For each of such states, all predecessors are considered and appropriate message that will cause assumption update is sent.
				
				If formula is marked as valid in state $s$ for color $p$, it means a message containing such state and color has been received. This can only happen if said state has a successor under color $p$ where $\varphi_1$ holds. Therefore no false positive results are produced. Also, for each state where $\varphi_1$ holds for color $p$, a message is sent for all predecessors of such state. Therefore all states where $\ex{\varphi_1}$ holds are labeled accordingly. Since all correct states are labeled and no false positives are possible, the algorithm is correct.
				
				In the worst case, algorithm has to send every color over every edge, therefore worst case message complexity is $card(\trans{}_{i})$. In practice, message count is usually much lower, since multiple colors can be packed into one message.
				
				Assuming the validity of $\varphi_1$ is computed for all states, function $validStates(\varphi_1, \as)$ can be computed in $O(|internal(S_{i})| \cdot |\params|)$. The function $predecessors$ can be pre-computed for all states in $O(card(\trans{}_{i}))$ time. The procedure \method{Send} is called at most $card(\trans{}_{i})$ times and the parameter set intersection is at worst linear in the size of parameter space. 
				
				This would result in $O(|\params| \cdot card(\trans{}_{i}))$ complexity. However, this can be further reduced to $O(card(\trans{}_{i}))$ since we can observe that for every predecessor where $|tranCol| > 1$, only one message is sent instead of $|tranCol|$ messages. The color set intersection can also be performed in $O(|tranCol|)$ time. This means that the price of set intersection is amortized by the reduced number of transmitted messages. 
				
				
			
    		\subsection{Exist Until Operator}
				\begin{algorithmic}[1]
				\State $ \textbf{Process variables:} $
				\State $ \ks = (id, f, \mathcal{P}, S, I, \trans{p}, L) $ \Comment{Kripke fragment}
				\State $ \phi = \eu{\phi_1}{\phi_2}  $ \Comment{CTL formula}
				\State $ \as $ \Comment{Initial assumption function}
				\Procedure{Init}{}
					\ForAll {$(state, colSet) $ \textbf{in} $ validStates(\phi_2, \as) $}
						\State $ \as \gets update(\as, state, colSet, \phi) $
						\ForAll { $ (pred, tranCol) $ \textbf{in} $ predecessors(state) $ }
							\State $ \Call{Send}{f(state), (pred, colSet \cap tranCol)} $
						\EndFor
					\EndFor
				\EndProcedure
				\Procedure{Receive}{$colSet, state$}
					\State $ colSet \gets colSet \cap valid(\phi_1, to, \as)$
					\If{$ colSet \neq \emptyset $ \textbf{and} $ colSet \setminus valid(\phi, to, \as) \neq \emptyset $}
						\State $ \as \gets update(\as, to, colSet, \phi) $
						\ForAll {$(pred, tranCol) $ \textbf{in} $ predecessors(to) $ }
							\State $ \Call{Send}{f(pred), (pred, colSat \cap tranCol)} $
						\EndFor
					\EndIf
				\EndProcedure
				\end{algorithmic}	
			
				The \eu{}{} operator is a little more complex, but again fairly simple. The algorithm starts by computing all states and colours where $\phi_2$ is true. Starting from these states, a backpropagation of parameter sets along the reversed transitions is performed. During the computation, the propagated parameter set is updated to reflect the validity of $\phi_1$ and the validity of transitions used along the path. Note that backpropagation is stoped as soon as there is no new information computed ($colSat$ is either empty or equal to already computed assumptions).
			
		    \subsection{All Until Operator}
				\begin{algorithmic}[1]
				\State $ \textbf{Process variables:} $
				\State $ \ks = (id, f, \mathcal{P}, S, I, \trans{p}, L) $ \Comment{Kripke fragment}
				\State $ \phi = \au{\phi_1}{\phi_2}  $ \Comment{CTL formula}
				\State $ T = \trans{p} $ \Comment{Uncovered edges}
				\State $ \as $ \Comment{Initial assumption function}
				\Procedure{Init}{}
					\ForAll {$(state, colSet) $ \textbf{in} $ validStates(\phi_2) $}
						\ForAll { $ (pred, tranCol) $ \textbf{in} $ predecessors(state) $ }
							\State $ \Call{Send}{f(state), (state, pred, \mathcal{P} \cap tranCol)} $
						\EndFor
					\EndFor
				\EndProcedure
				\Procedure{Receive}{$colSet, from, to$}	
					\State $ T \gets T \setminus \{ (to, p, from) | p \in colSet \} $
					\State $ colSet \gets \{ p | p \in colSet \wedge \forall s_2 \in S. (to, p, s_2) \notin T \} $
					\State $ colSet \gets colSet \cap valid(\phi_1, to, \as)$
					\If{$ colSet \neq \emptyset $ \textbf{and} $ colSet \setminus valid(\phi, to, \as) \neq \emptyset $}
						\State $ \as \gets update(\as, to, colSet, \phi) $
						\ForAll {$(pred, tranCol) $ \textbf{in} $ predecessors(to) $ }
							\State $ \Call{Send}{f(pred), (to, pred, colSet \cap tranCol)} $
						\EndFor
					\EndIf
				\EndProcedure
				\end{algorithmic}	
				
				The \au{}{} operator is the most complex one to handle. As opposed to \ex{}{}, which requires at least one valid successor to be true, \au{}{} requires that all successors of the specific node are valid. In order to compute such information, we create a copy of transition relation and call it $T$. 
				
				During the computation, $T$ is modified in such way, so that we can guarantee that if edge is not present in $T$, this edge leads to a state where either $\phi_2$ or \au{\phi_1}{\phi_2} holds. This way, we can guarantee that only appropriate states and colors are marked as valid by our algorithm.
				
								
		\section{Merge message buffer}
			
			The main argument of coloured model checking efficiency is based on the assumption that operations on parameter sets are in practice less expensive than graph traversal. However, even a simple model structure can easily break the computation into many simultanious traversals. When two different colour sets are marked as valid in a state due to two distinct transitions, unfortunate timing can easily prevent these two colour sets from merging into one. This leads to two outgoing messages instead of one, which in the end results in two separate graph traversals instead of one. 
			
			In some cases, this simply cannot be avoided, since in order to know exactly when to wait for a merge and when to continue the traversal, we would basically have to solve the coloured model checking problem. However, we can take advantage of the fact, that many messages cannot be processed directly at the time of arrival, since message processing can be quite costly operation, especially when the transition system is being lazily calculated. Therefore we use a message buffer to store incomming messages that cannot be processed directly.
			
			These bufferes can grow quite large during the computation and even outgrow the state space of the transition system itself. Of course, it would be easy to just slow down the creation of messages to prevent the buffers from growing. However, many messages in these buffers contain data that are either duplicate or can be merged into one message while maintaining correct semantics.
			
			In order to reduce to the number of unnecessary graph traversals and reduce memory footprint of message buffers while maintaining good performance, we employ a merge message buffer as described in [REFERENCE]. 
			
			In this text, we only provide the implementation for messages that contian destination node and colour set as used in $\ex{}$ and $\eu{}{}$ operators. However, the implementation for messages used by $\au{}{}$ operator can be obtained trivially by replacing all occurences of destination node with pair of destination and source nodes.
			
			\begin{algorithmic}[1]
			\State $ \textbf{MergeBuffer} $
			\State $ Q \gets IterableHashMap $
			\Procedure{Insert}{node, colours}
				\If {$Q$ \textbf{hasKey} $node$}
					\State $current \gets Q.get(node)$
					\State $Q.replace(node, colours \cup current)$
				\Else
					\State $Q.insert(node, colours)$				
				\EndIf
			\EndProcedure
			\Procedure{Empty}{}
				\State \Return $Q.isEmpty()$
			\EndProcedure
			\Procedure{Take}{}
				\State $val \gets Q.interator().first()$
				\State $Q.remove(val.key)$
				\State \Return $val$
			\EndProcedure
			\end{algorithmic}				
			
			Key property of the buffer is that it is backed by an $IterableHashMap$. Compared to traditional $HashMap$, $IterableHashMap$ provides also an iterator on all of its key-value pairs. This gives us the ability to take one (non-deterministic) element out of the map in constant time. Considering a reasonable hash function on the node set, we can guarantee that each operation on the underlying $IterableHashSet$ can be done in constant time. Therefore the only interesting operation in terms of time complexity is the union of two colour sets, which can be done in $O(\params)$ complexity.
			
			It is important to note that the resulting buffer does not preserve the FIFO properties of a queue. However, this is not required by the algorithm (actually, performing a random search instead of classic DFS is considered a valid optimalization heuristic).
			
			We do not provide exhaustive benchmark of this heuristic, since the main priority of this work is the model checking algorithm itself. However, a comparison with linked queue and circular buffer on models tested in the Scalability section showed that merge queue easily outperforms both of them especially in highly distributed environments. Linked queue was usually two times slower while circular buffer managed to provide approximately 60-80\% of the merge buffer performance. 
			
			One reason for this is the fact that more distributed computations have generally more cross transitions. This increases the cost of traversal compared to parameter set operations which in turn makes the merge buffer more effective. Also, the properties of this buffer allow in some cases for super-linear scalablilty, since greater number of processes can produce higher number of merged traversals. 
			
							
	
	\chapter{Implementation}

		Our algorithm is implemented in a proof-of-concept distributed CTL model checker available in the github repositiory of Sybila organization [cite]. In this section we briefly discuss the implementation details of this project. 
		
		Although most of the core model checking module is fully operational and stable, the project is still mainly in experimental phase, since new features, heuristics and modelling approaches are still being considered and reevaluated.

		\section{Architecture}
		
			In order to provde greater flexibility and ease of development, majority of the project is implemented in Java. However, several parts still require C++ code reused from similar previous projects. 
			
			The distributed environment is mainly provided by MPJ [cite], Java implementation of standard MPI interface. However, the model checking algorithm itself does not depend on any concrete communication library and can be easily adapted to any similar distributed communication tool.
			
			The whole project is divided into several modules in order to maximize extensibility and modifiability during future development. This section provides a quick overview of each of these modules.
			
			\subsection{CTL Parser}
			
				In order to provide user with easy way to input CTL formulas, this module implements a parser for modified formula specification language used in BioDivIne[cite]. 
				
				The original parser only supports LTL formulas, therefore the grammar has been modified to allow for CTL operators. However, user familiar with the original LTL syntax should have no problems adapting to the CTL syntax.
				
				The grammar is written in Antlr parser generator and apart from the whole range of CTL and boolean operators supports also boolean and float propositions.
				
				This module also handles the transformation of user provided formula into minimal set of operators supported by the main model checker.
				
			\subsection{Model Checker}
			
				This is the core module implementing the model checking algorithm described in this work. It defines the interfaces and contracts representing the kripke fragment, parameter set, partition function and inter-process communication.
				
				It is completely independant on the implementation of said interfaces, and therefore provides great flexibility and extensibility. This module also provides partial implementations of some of these interfaces to ease the development of model related modules. 
				
				This module also implements the termination detection algorithm designed by Safra[cite]. The algorithm itself can't be easily overriden, however, the communication is again isolated into one, easily replacable class, so the support for different communcation libraries can be easily provided.
	
			\subsection{ODE Abstraction}
			
				This module is based on the ODE state space generator from the LTL model checking tool BioDiViNe. The code responsible for model parsing and abstraction calculation is reused directly and Java Native Interface(JNI) is used to extract resulting model into corresponding Java data structures.
				
				The state space generator responsible for evaluation of atomic propositions and computation of successors/predecessors is also based on code from the BioDiViNe project. However, this section has been completely rewritten into Java and adapted to the CTL paradigm. This minimizes the number of slow JNI calls to existing C++ code. This new state space generator also features several bugfixes and speed optimalizations.
				
				This module also implements the rectangular state space partition function described in this work. The parameter set is implemented using Google Guava Range Set[cite] which provides great flexibility and huge feature set while maintaining good performance.
				
			\subsection{Thomas Network Abstraction}

				This module is based on the Thomas Network state space generator from the LTL model checking tool Parsybone. Similarly to the ODE Abstraction module, most of the model parsing and preprocessing is done using the original code and extracted using JNI. 
				
				The parameter set is implemented using a EWAH Compressed Bitmap[cite], which provides decent performance even on large parameter sets and is very easy to use. 
				
				At the moment of writing, this module does not provide any good partitioning implementation, since much of this functionality is still in developement and most of the current implementation is subject to change. However, sequential computation on one processing node is fully supported.
				
			\subsection{Frontend}			
			
				Frontend module ties together the functionality of all modules into several runnable utilities. However, the documentation and general outilne and output format of these tools have not yet been finalized and is subject to change. Therfore the code in this module should be taken more as an example of usage of different modules.		
		
					
    \appendix
    \chapter{First appendix}        % Appendices
    \Blindtext
    \chapter{Another appendix}
    \Blindtext

    \bibliographystyle{ieeetr}  
	\bibliography{bibliography}
    % Bibliography goes here
    % Index goes here (optional)
\end{document}
